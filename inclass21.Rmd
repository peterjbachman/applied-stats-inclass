---
title: "Applied Statistical Programming - The EM Algorithm"
date: "4/13/2022"
author: "Amaan, Alex, Patrick, Peter"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{geometry}
   - \usepackage{hyperref}
   - \usepackage{setspace}
   - \usepackage{hyperref}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{=tex}
\textbf{Write R and Rcpp code to answer the following questions. Write the code,
  and then show what the computer returns when that code is run. Thoroughly
  comment your solutions.}
```
Complete this assignment before 10:00am on Wednesday, April 20. Submit the R
implementation as an Rmarkdown and the knitted PDF to Canvas. Have one group
member submit the activity with all group members listed at the top. The Rcpp
portion will be given to you as your final assignment.

\section*{In-class Background: The Expectation-Maximization Algorithm}

The goal of this in-class exercise is to implement an ensemble of models. You
will combine forecasts of US presidential elections using ensemble Bayesian
model averaging (EBMA). To do this, you must decide how to weight each component
of the forecast in the prediction. The collection of these weighted forecasts
form the ensemble, and you will use something called the EM
(expectation-maximization) algorithm.

The task is to choose values $w_k$ that maximize the following equation:

```{=tex}
\begin{equation}
   p(y \vert f_1^{s \vert t^{\star}}, ..., f_K^{s \vert t^{\star}}) = \sum\limits^N_{k=1} w_k N(f_k^{t^{\star}}, \sigma^{2})
\end{equation}
```
For the remainder of this assignment, assume that the parameter $\sigma^{2}$ is
known and that $\sigma^{2} = 1$.

The first step of the EM algorithm is to estimate the latent quantity
$\hat{z}^t_k$ that represents the probability that observation $t$ was best
predicted by model $k$.

```{=tex}
\begin{equation}
   \hat{z}_k^{(j+1)t} = \frac{\hat{w}_k^{(j)} N(y^t \vert f_k^t, 1)}{\sum\limits^N_{k=1} \hat{w}_k^{(j)} N(y^t \vert f_k^t, 1)}
\end{equation}
```
In this equation, $j$ is the particular iteration of the EM algorithm, and
$N(y^t \vert f_k^t, 1)$ is the normal cumulative distribution function evaluated
at the observed election outcome (\texttt{dnorm(y, ftk, 1)}).

The second step of the EM algorithm is to estimate the expected value of the
weights assuming that all $\hat{z}^{t}_{k}$ are correct.

```{=tex}
\begin{equation}
   \hat{w}^{(j+1)}_k = \frac{1}{n} \sum_t \hat{z}_k^{(j+1)t}
\end{equation}
```
\newpage

The estimation procedure is as follows:

```{=tex}
\begin{enumerate}
   \item Start with the assumption that all models are weighted equally.
   \item Calculate $\hat{z}_k^{(j+1)t}$ for each model for each election.
   \item Calculate $\hat{w}_k^{(j+1)}$ for each model.
   \item Repeat steps 2-3 twenty times.
\end{enumerate}
```
Complete the preceding tasks in \texttt{R} alone.

# **ANSWERS**: R Section.

**FIRST**: find test data to use on our model.

  * we use the Expectation-Maximization (EM) algorithm on datasets that combine multiple linear models. In this case, we combine different forecasts of US presidential elections using EBMA.

  * To learn more about EBMA forecasts and (hopefully) find some toy data to use with our functions, let's install and investigate the `EBMAforecast` package.

```{r tidy = TRUE}
## Install package:

#install.packages("EBMAforecast")
library(EBMAforecast)

## Read documentation:

#?EBMAforecast

## Find data used in demo(PresForecast):

#demo(presForecast)
```

  * Let's load the `presidentialForecast` data from the `EBMAforecast` package:

```{r tidy = TRUE}
## Load `presidentialForecast` data from `EBMAforecast` package:
data("presidentialForecast")
#?presidentialForecast

## OUTCOME VARIABLE: incumbent-party vote share in each presidential election.
```

**SECOND**: for the remaining R code, let's first develop code for each of the steps referenced above before properly structuring them in a for-loop.

1. **Start with the assumption that all models are weighted equally**.

**NOTE**: recall from Montgomery, Hollenbach, and Ward (2012) that the $w_k \in [0,1]$'s are model probabilities associated with each component model's predictive performance.
In other words, the $w_k \in [0,1]$'s are weights associated each each model such that $\sum_{k=1}^K w_k = 1$.

```{r tidy = TRUE}
## ASSUMED DATA STRUCTURE: dataframe with:

### `K` columns of models. 

### `n` rows of predictions (substantively, these are the predicted incombent-party vote share for each presidential election).
```

2. **Calculate $\hat{z}_k^{(j+1)t}$ for each model for each election**.

```{r}

```

3. **Calculate $\hat{w}_k^{(j+1)}$ for each model**.

```{r}

```

**THIRD**: perform step (4) above by creating a for-loop that repeats steps 2-3 twenty times.

```{r}

```





```{r eval = FALSE}
# Eventually wrap everything in a loop, or apply function?

# Calculate zHat

# I'm not sure if the indices are in the right places. This is mostly because
# I'm not entirely sure what's going on
zHat[i + 1] <- (wHat[i] * dnorm(y[i], ftk[i], 1)) / sum(wHat * dnorm(y, ftk, 1))

# Calculate wHat

# Not sure for the same reasons as above.
wHat[i + 1] <- sum(zHat) / length(zHat)

```

\section*{Assignment: Rcpp Practice}

```{=tex}
\begin{enumerate}
   \item Write an Rcpp function that will calculate the answer to Equation (2).
     The output will be a matrix.
   \item Write an Rcpp function that will calculate the answer to Equation (3).
     The output will be a vector.
   \item Write an Rcpp function that will complete the entire algorithm.
\end{enumerate}
```
```{r rcppVersion}

```

